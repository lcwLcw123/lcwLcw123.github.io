{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7v2dPqfZFIV"
      },
      "source": [
        "#### Libraries Imported and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3bQIxfD9Ep0y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YS7R221iUMI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU:  NVIDIA GeForce RTX 3060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc5EfTgZIBk"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75W0devEPY7"
      },
      "source": [
        "##### Vision Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention Module used to perform self-attention operation allowing the model to attend\n",
        "    information from different representation subspaces on an input sequence of embeddings.\n",
        "    The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Query, Key, Value -> ReshapeHeads -> Query.TransposedKey -> Softmax -> Dropout\n",
        "    -> AttentionScores.Value -> ReshapeHeadsBack -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        _reshape_heads(inp) :- \n",
        "        Changes the input sequence embeddings to reduced dimension according to the number\n",
        "        of attention heads to parallelize attention operation\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size * heads, seq_len, reduced_dim)\n",
        "\n",
        "        _reshape_heads_back(inp) :-\n",
        "        Changes the reduced dimension due to parallel attention heads back to the original\n",
        "        embedding size\n",
        "        (batch_size * heads, seq_len, reduced_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        forward(inp) :-\n",
        "        Performs the self-attention operation on the input sequence embedding.\n",
        "        Returns the output of self-attention as well as atttention scores\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim), (batch_size * heads, seq_len, seq_len)\n",
        "\n",
        "    Examples:\n",
        "        >>> attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        >>> out, weights = attention(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "        \n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XYemII1gElcK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        # Apply first FC layer and GELU activation\n",
        "        activated_output = self.activation(self.fc1(inp))\n",
        "\n",
        "        # Create a mask where each position that has a value greater than 0 is set to 1\n",
        "        positive_mask = (activated_output > 0).float() # Convert boolean mask to float\n",
        "\n",
        "        # Optionally, apply the mask to set values greater than 0 to 1\n",
        "        # Commented out since it's not clear if you want to replace the values or just record the mask\n",
        "        # activated_output = activated_output * positive_mask\n",
        "\n",
        "        # Apply dropout and second FC layer\n",
        "        out = self.dropout(activated_output)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        # positive_mask: Tensor with 1s where GELU activation was positive, 0s elsewhere\n",
        "        return out, positive_mask\n",
        "\n",
        "# Example usage:\n",
        "# FF = FeedForward(8, 1)\n",
        "# out, positive_mask = FF(inp)\n",
        "# print(\"Mask of positive GELU activations:\", positive_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uMrCi7DNGvCc"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''\n",
        "    Transformer Block combines both the attention module and the feed forward module with layer\n",
        "    normalization, dropout and residual connections. The sequence of operations is as follows :-\n",
        "    \n",
        "    Input -> LayerNorm1 -> Attention -> Residual -> LayerNorm2 -> FeedForward -> Output\n",
        "      |                                   |  |                                      |\n",
        "      |-------------Addition--------------|  |---------------Addition---------------|\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> TB = TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = TB(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        res = inp\n",
        "        out = self.norm1(inp)\n",
        "        out, _ = self.attention(out)\n",
        "        out = out + res\n",
        "        \n",
        "        res = out\n",
        "        out = self.norm2(out)\n",
        "        out, positive_mask = self.feed_forward(out)\n",
        "        out = out + res\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out,positive_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kcBcReoPJ4NC"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    '''\n",
        "    Transformer combines multiple layers of Transformer Blocks in a sequential manner. The sequence\n",
        "    of the operations is as follows -\n",
        "\n",
        "    Input -> TB1 -> TB2 -> .......... -> TBn (n being the number of layers) -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = transformer(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trans_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
        "        )\n",
        "        self.all_positive_masks = []\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        self.all_positive_masks = []\n",
        "        out = inp\n",
        "        for block in self.trans_blocks:\n",
        "            out, positive_mask  = block(out)\n",
        "            self.all_positive_masks.append(positive_mask)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out,self.all_positive_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XRN2k5-kLWYG"
      },
      "outputs": [],
      "source": [
        "# Not Exactly Same as Paper\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''\n",
        "    Classification Head attached to the first sequence token which is used as the arbitrary \n",
        "    classification token and used to optimize the transformer model by applying Cross-Entropy \n",
        "    loss. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classification classes in the dataset\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, embed_dim) -> (batch_size, classes)\n",
        "\n",
        "    Examples:\n",
        "        >>> CH = ClassificationHead(embed_dim, classes, dropout)\n",
        "        >>> out = CH(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.softmax(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, classes) \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M-8nXI7gPxIs"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    '''\n",
        "    Vision Transformer is the complete end to end model architecture which combines all the above modules\n",
        "    in a sequential manner. The sequence of the operations is as follows -\n",
        "\n",
        "    Input -> CreatePatches -> ClassToken, PatchToEmbed , PositionEmbed -> Transformer -> ClassificationHead -> Output\n",
        "                                   |            | |                |\n",
        "                                   |---Concat---| |----Addition----|\n",
        "    \n",
        "    Args:\n",
        "        patch_size: Length of square patch size \n",
        "        max_len: Max length of learnable positional embedding\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classes in the dataset\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        channels: Number of channels in the input (Default=3)\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        It outputs the classification output as well as the sequence output of the transformer\n",
        "        (batch_size, channels, width, height) -> (batch_size, classes), (batch_size, seq_len+1, embed_dim)\n",
        "    \n",
        "    Examples:\n",
        "        >>> ViT = VisionTransformer(atch_size, max_len, embed_dim, classes, layers, channels, heads, activation, forward_expansion, dropout)\n",
        "        >>> class_out, hidden_seq = ViT(inp)\n",
        "    '''\n",
        "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, channels=3, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.name = 'VisionTransformer'\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.channels = channels\n",
        "        self.patch_to_embed = nn.Linear(patch_size * patch_size * channels, embed_dim)\n",
        "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
        "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.all_positive_masks =[]\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, channels, width, height)\n",
        "        batch_size, channels, width, height = inp.size()\n",
        "        assert channels == self.channels\n",
        "        self.all_positive_masks =[]\n",
        "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
        "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
        "        out = out.permute(0, 2, 3, 4, 1)\n",
        "        # out: (batch_size, seq_len, patch_size, patch_size, channels) | seq_len would be (width*height)/(patch_size**2)\n",
        "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
        "        \n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.patch_to_embed(out)\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat([class_token, out], dim=1)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "        position_embed = self.position_embed[:seq_len+1]\n",
        "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
        "        out = out + position_embed\n",
        "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
        "\n",
        "        out,self.all_positive_masks = self.transformer(out)\n",
        "        # out: (batch_size, seq_len+1, embed_dim) \n",
        "        class_token = out[:, 0]\n",
        "        # class_token: (batch_size, embed_dim)\n",
        "\n",
        "        class_out = self.classification_head(class_token)\n",
        "        # class_out: (batch_size, classes)\n",
        "        \n",
        "        return class_out, out,self.all_positive_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGS4bmCRWmIg"
      },
      "source": [
        "#### Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "60fXexWWWlpJ"
      },
      "outputs": [],
      "source": [
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
        "    '''\n",
        "    A wrapper function that creates a DataLoader for CIFAR100 dataset loaded from torchvision using \n",
        "    the parameters supplied and applies the required data augmentations.\n",
        "\n",
        "    Args:\n",
        "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
        "        batch_size: Batch size to used for loading data (Default=8)\n",
        "        num_workers: Number of parallel workers used to load data (Default=2)\n",
        "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
        "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
        "        normalize: A string to decide the normalization to applied to the input images\n",
        "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
        "    \n",
        "    Output:\n",
        "        DataLoader Object\n",
        "    '''\n",
        "    if normalize == 'imagenet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif normalize == 'standard':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std =  [0.5, 0.5, 0.5]\n",
        "\n",
        "    if split == 'train':\n",
        "        if size == '224':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop((224,224), scale=(0.8, 1.0)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        elif size == '32':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(15),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        \n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        if size == '224':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        elif size == '32':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_YjSG0aLJR"
      },
      "source": [
        "#### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Byfz7zrEaKxq"
      },
      "outputs": [],
      "source": [
        "# Initializations of all the constants used in the training and testing process\n",
        "\n",
        "lr = 0.003\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "shuffle = True\n",
        "patch_size = 4\n",
        "image_sz = 32\n",
        "max_len = 100 # All sequences must be less than 1000 including class token\n",
        "embed_dim = 512\n",
        "classes = 100\n",
        "layers = 12\n",
        "channels = 3\n",
        "resnet_features_channels = 64\n",
        "heads = 16\n",
        "epochs = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d89pJGwZaUBs"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler, resnet_features=None):\n",
        "    '''\n",
        "    Function used to train the model over a single epoch and update it according to the\n",
        "    calculated gradients.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        optimizer: Optimizer used update the model\n",
        "        scheduler: Scheduler used to update the learing rate for faster convergence \n",
        "                   (Commented out due to poor results)\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "\n",
        "    Output:\n",
        "        running_loss: Training Loss (Float)\n",
        "        running_accuracy: Training Accuracy (Float)\n",
        "    '''\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    # If the accumulator is None, initialize it with the size of the positive_mask\n",
        "    all_feedforward_modules = [module for module in model.modules() if isinstance(module, FeedForward)]\n",
        "    # positive_mask_accumulators = [torch.zeros_like(ff.fc1.weight.data) for ff in all_feedforward_modules]\n",
        "    positive_mask_accumulators = [torch.zeros_like(torch.ones([512]).to(device)) for ff in all_feedforward_modules]\n",
        "    # max_batches = 32\n",
        "    # batch_count = 0\n",
        "    # Accumulate the positive masks\n",
        "    for data, target in tqdm(dataloader):\n",
        "        # if batch_count >= max_batches:\n",
        "        #     break  # Exit the loop after processing 32 batches\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if model.name == 'VisionTransformer':\n",
        "            with torch.no_grad():\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "            output, _ ,all_positive_masks= model(data)\n",
        "        elif model.name == 'ResNet':\n",
        "            output = model(data)\n",
        "        for i, positive_mask in enumerate(all_positive_masks):\n",
        "            positive_mask_accumulators[i] += positive_mask.sum(0).sum(0)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(dataloader)\n",
        "        running_loss += loss.item() / len(dataloader)\n",
        "        # batch_count += 1\n",
        "\n",
        "        # Flatten the accumulator masks\n",
        "    flattened_masks = torch.cat([mask.flatten() for mask in positive_mask_accumulators])\n",
        "\n",
        "    # Determine the threshold value for the smallest 20%\n",
        "    kth_value = int(len(flattened_masks) * 0.05)\n",
        "    threshold_value, _ = torch.kthvalue(flattened_masks, kth_value)\n",
        "\n",
        "    \n",
        "    # Loop over each FeedForward module and its corresponding mask accumulator\n",
        "    for ff_module, mask_accumulator in zip(all_feedforward_modules, positive_mask_accumulators):\n",
        "        small_values_mask = mask_accumulator < threshold_value\n",
        "        \n",
        "        with torch.no_grad():  # Ensure no gradients are computed for this operation\n",
        "\n",
        "            rows_to_randomize = small_values_mask\n",
        "            ff_module.fc1.weight.data[rows_to_randomize] = torch.randn_like(ff_module.fc1.weight.data[rows_to_randomize])\n",
        "\n",
        "            # For fc2, this involves columns in the weight matrix, because it's connected to the output of fc1\n",
        "            cols_to_randomize = small_values_mask\n",
        "            ff_module.fc2.weight.data[:, cols_to_randomize] = torch.randn_like(ff_module.fc2.weight.data[:, cols_to_randomize])\n",
        "\n",
        "            ff_module.fc1.bias.data[rows_to_randomize] = torch.randn_like(ff_module.fc1.bias[rows_to_randomize])\n",
        "            ff_module.fc2.bias.data[cols_to_randomize] = torch.randn_like(ff_module.fc2.bias[cols_to_randomize])\n",
        "\n",
        "    \n",
        "    return running_loss, running_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dXAFKy14EIyU"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader, criterion, resnet_features=None):\n",
        "    '''\n",
        "    Function used to evaluate the model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "    \n",
        "    Output:\n",
        "        test_loss: Testing Loss (Float)\n",
        "        test_accuracy: Testing Accuracy (Float)\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        test_accuracy = 0.0\n",
        "        test_loss = 0.0\n",
        "        for data, target in tqdm(dataloader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if model.name == 'VisionTransformer':\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "                output, _,all_positive_masks= model(data)\n",
        "            elif model.name == 'ResNet':\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc = (output.argmax(dim=1) == target).float().mean()\n",
        "            test_accuracy += acc / len(dataloader)\n",
        "            test_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1NNc-FxvJU7"
      },
      "source": [
        "#### Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FIkGknvMUW"
      },
      "source": [
        "Run either one the following subcells according to the models selected to train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAkCvMzfN3R"
      },
      "source": [
        "##### Model - Vision Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVBkOJnnvWzH"
      },
      "source": [
        "Recommended Values for the following Architecture\n",
        "\n",
        "- patch_size = 4\n",
        "- max_len = 100\n",
        "- embed_dim = 512\n",
        "- classes = According to Dataset\n",
        "- layers = 12\n",
        "- channels = 3\n",
        "- heads = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WG_wT7pXe5q9"
      },
      "outputs": [],
      "source": [
        "# Vision Transformer Architecture\n",
        "\n",
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    channels=channels,\n",
        "    heads=heads).to(device)\n",
        "\n",
        "resnet_features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0x9qE4HhUhR"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yDEtyzw8oU"
      },
      "source": [
        "##### CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pp1l68Z_UiGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [02:29<00:00,  2.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1 - acc: 0.0100 - loss : 4.6064\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:11<00:00,  6.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.0097 - test loss : 4.6680\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [02:31<00:00,  2.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 2 - acc: 0.0100 - loss : 4.6062\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:10<00:00,  7.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.0107 - test loss : 4.6821\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [02:29<00:00,  2.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 3 - acc: 0.0100 - loss : 4.6063\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:11<00:00,  6.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.0106 - test loss : 4.6168\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [02:28<00:00,  2.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 4 - acc: 0.0100 - loss : 4.6063\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:11<00:00,  7.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.0105 - test loss : 4.6058\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [02:29<00:00,  2.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 5 - acc: 0.0100 - loss : 4.6057\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:11<00:00,  6.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.0105 - test loss : 4.8019\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 209/391 [01:20<01:10,  2.59it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mf:\\america\\Vision-Transformer\\VisionTransformer copy 2.ipynb 单元格 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_accs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     running_loss, running_accuracy \u001b[39m=\u001b[39m train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch : \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m - acc: \u001b[39m\u001b[39m{\u001b[39;00mrunning_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m - loss : \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_accs\u001b[39m.\u001b[39mappend(running_accuracy)\n",
            "\u001b[1;32mf:\\america\\Vision-Transformer\\VisionTransformer copy 2.ipynb 单元格 28\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# scheduler.step()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/america/Vision-Transformer/VisionTransformer%20copy%202.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m acc \u001b[39m=\u001b[39m (output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m target)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32me:\\ANACONDA\\envs\\opencv\\lib\\site-packages\\torch\\optim\\adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[0;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=0, shuffle=shuffle, size='32', normalize='standard')\n",
        "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=0, shuffle=False, size='32', normalize='standard')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    train_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "    if (epoch)%5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_acc': train_accs,\n",
        "            'test_acc': test_accs\n",
        "        }, '\\checkpoints' + model.name + '_CIFAR100_checkpoint_new.pt') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KjgBoZoxU42"
      },
      "source": [
        "##### Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsB1V8Gl0jnQ"
      },
      "outputs": [],
      "source": [
        "train_accs = [acc.cpu().item() for acc in train_accs]\n",
        "test_accs = [acc.cpu().item() for acc in test_accs]\n",
        "# print(train_accs)\n",
        "# print(test_accs)\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(range(1, 101), train_accs, label='Train Accuracy')\n",
        "plt.plot(range(1, 101), test_accs, label='Test Accuracy')\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.title(\"Train vs Test Accuracy\")\n",
        "plt.legend(loc='lower right')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNCZTp0j+sS0hGspyPCbLz6",
      "include_colab_link": true,
      "mount_file_id": "194YTo4VqBt4HXEGcBz__l49llDopmNMP",
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
